{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Set Up & Import__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python\n",
    "# %pip install mediapipe\n",
    "# %pip install plotly\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theerat/Documents/tsl/sign-language-translator/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Color Class [ _Not Important_ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PINK = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   PURPLE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732439418.200276  130942 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1732439418.329169  134157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.344696  134157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.348378  134153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.348968  134156 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.349114  134159 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.361790  134158 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.371998  134153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732439418.372441  134156 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.1,\n",
    "    min_tracking_confidence=0.1\n",
    ")\n",
    "\n",
    "# Initializing the drawing utils for drawing the landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Define Needed Function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract & Structerize Position From Each Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(hand_pos):\n",
    "    right, left, body, face, i = dict(), dict(), dict(), dict(),0\n",
    "    if hand_pos[0] != None:\n",
    "        for data in hand_pos[0].landmark:\n",
    "            right['Landmark '+str(i)] = [data.x, data.y, data.z]\n",
    "            i+=1\n",
    "        i = 0\n",
    "    else:\n",
    "        right = None\n",
    "\n",
    "    if hand_pos[1] != None:\n",
    "        for data in hand_pos[1].landmark:\n",
    "            left['Landmark '+str(i)] = [data.x, data.y, data.z]\n",
    "            i+=1\n",
    "        i = 0\n",
    "    else:\n",
    "        left = None\n",
    "\n",
    "    if hand_pos[2] != None:\n",
    "        for data in hand_pos[2].landmark:\n",
    "            body['Landmark '+str(i)] = [data.x, data.y, data.z]\n",
    "            i+=1\n",
    "        i = 0\n",
    "    else:\n",
    "        body = None\n",
    "\n",
    "    if hand_pos[3] != None:\n",
    "        for data in hand_pos[3].landmark:\n",
    "            face['Landmark '+str(i)] = [data.x, data.y, data.z]\n",
    "            i+=1\n",
    "        i = 0\n",
    "    else:\n",
    "        face = None\n",
    "\n",
    "    hand_pos = {'Right': right, 'Left': left, 'Body': body, 'Face': face}\n",
    "    hand_pos_df = pd.DataFrame(hand_pos).T\n",
    "    \n",
    "    return hand_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate(mp_holistic,holistic_model, mp_drawing, file):    \n",
    "    vid = cv2.VideoCapture(file)\n",
    "    total_frame, total_capture = int(vid.get(cv2.CAP_PROP_FRAME_COUNT)), 0\n",
    "\n",
    "    for i in range(total_frame):\n",
    "        if i%(total_frame//30) == 0:\n",
    "            total_capture+=1\n",
    "        else:\n",
    "            continue\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        vid.set(cv2.CAP_PROP_POS_FRAMES, (total_capture)*(total_frame/30))\n",
    "\n",
    "        if not ret or cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        results = holistic_model.process(image)\n",
    "        image.flags.writeable = True\n",
    "    \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw Pose And Face Land Marks \n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        # Drawing Right hand Land Marks\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        # Drawing Left hand Land Marks\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        # Draw Face Land Marks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow(\"Display\", image)\n",
    "\n",
    "        left= results.left_hand_landmarks\n",
    "        right = results.right_hand_landmarks\n",
    "        upper = results.pose_landmarks\n",
    "        face = results.face_landmarks\n",
    "        hand_pos_df = extract_data([right, left, upper, face])\n",
    "        os.makedirs(file.split('.')[0], exist_ok=True)\n",
    "        hand_pos_df.to_csv(file.split('.')[0]+'/Frame_'+str(total_capture-1)+'.csv')\n",
    "        print(\"\\r\"+\"Successfully captured frame:\", color.RED+color.BOLD+str(total_capture)+color.END+\" / 30\", end=\"\")\n",
    "        # print(\"Successfully captured frame:\", color.RED+color.BOLD+str(total_capture)+color.END+\" / 30\")\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\n\"+color.BOLD+color.GREEN+\"Done!\"+color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data2(hand_pos):\n",
    "    data = {\"Right\": [], \"Left\": [], \"Body\": [], \"Face\": []}\n",
    "    keys = [\"Right\", \"Left\", \"Body\", \"Face\"]\n",
    "\n",
    "    for idx, landmarks in enumerate(hand_pos):\n",
    "        if landmarks:\n",
    "            data[keys[idx]] = [[lm.x, lm.y, lm.z] for lm in landmarks.landmark]\n",
    "        else:\n",
    "            data[keys[idx]] = None\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate_test(mp_holistic, holistic_model, mp_drawing, file):    \n",
    "    vid = cv2.VideoCapture(file)\n",
    "    total_frame = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total_capture = 0\n",
    "\n",
    "    os.makedirs(file.split('.')[0], exist_ok=True)  # สร้างโฟลเดอร์สำหรับบันทึกผลลัพธ์\n",
    "\n",
    "    for i in range(30):  # จับภาพ 30 เฟรม\n",
    "        frame_position = int((i / 30) * total_frame)\n",
    "        vid.set(cv2.CAP_PROP_POS_FRAMES, frame_position)\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # แปลงภาพเป็น RGB เพื่อให้ใช้กับ MediaPipe ได้\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = holistic_model.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # วาดจุด landmark บนภาพ\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "\n",
    "        # แสดงภาพที่มี landmark\n",
    "        cv2.imshow(\"Display\", image)\n",
    "\n",
    "        # ดึงข้อมูลจากจุด landmark\n",
    "        hand_pos_df = extract_data([\n",
    "            results.right_hand_landmarks,\n",
    "            results.left_hand_landmarks,\n",
    "            results.pose_landmarks,\n",
    "            results.face_landmarks\n",
    "        ])\n",
    "\n",
    "        # บันทึกข้อมูลเป็น CSV\n",
    "        hand_pos_df.to_csv(f\"{file.split('.')[0]}/Frame_{i}.csv\")\n",
    "        print(f\"\\rSuccessfully captured frame: {i+1}/30\", end=\"\")\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732351650.361916  232932 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully captured frame: 30/30\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "operate_test(mp_holistic,holistic_model, mp_drawing, '/Users/theerat/Documents/tsl/sign-language-translator/New25/ควาย/video_2.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = {\n",
    "        \"left_hand\": [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4), (0, 5), (0, 17), (5, 6), (6, 7),\n",
    "            (7, 8), (5, 9), (9, 10), (10, 11), (11, 12), (9, 13), (13, 14),\n",
    "            (14, 15), (15, 16), (13, 17), (17, 18), (18, 19), (19, 20)\n",
    "        ],\n",
    "        \"right_hand\": [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4), (0, 5), (0, 17), (5, 6), (6, 7),\n",
    "            (7, 8), (5, 9), (9, 10), (10, 11), (11, 12), (9, 13), (13, 14),\n",
    "            (14, 15), (15, 16), (13, 17), (17, 18), (18, 19), (19, 20)\n",
    "        ],\n",
    "        \"pose\": [\n",
    "            (8, 6), (6, 5), (6, 4), (4, 0), (0, 1), (1, 2), (2, 3), (3, 7),\n",
    "            (10, 9), (11, 12), (11, 13), (11, 23), (13, 15), (15, 21), (15, 17),\n",
    "            (15, 19), (17, 19), (12, 14), (12, 24), (14, 16), (16, 22), (16, 20),\n",
    "            (16, 18), (18, 20), (23, 24)\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_edges(ax, points, edges):\n",
    "        for edge in edges:\n",
    "            start, end = edge\n",
    "            x_values = [points[start][0], points[end][0]]\n",
    "            y_values = [points[start][1], points[end][1]]\n",
    "            ax.plot(x_values, y_values, 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_drawer(word, frames ,edges):\n",
    "    frame = pd.read_csv(word+'/frame_'+str(frames)+'.csv')\n",
    "        \n",
    "    # Extract data from the CSV file\n",
    "    lst_frame_right = frame.iloc[0][1:].to_frame().dropna().values\n",
    "    lst_frame_left = frame.iloc[1][1:].to_frame().dropna().values\n",
    "    lst_frame_body = frame.iloc[2][1:].to_frame().dropna().values\n",
    "    lst_frame_face = frame.iloc[3][1:].to_frame().dropna().values\n",
    "\n",
    "    # Convert string representations of lists to actual lists\n",
    "    lst_frame_right = [ast.literal_eval(lst_frame_right[i][0]) for i in range(len(lst_frame_right))]\n",
    "    lst_frame_left = [ast.literal_eval(lst_frame_left[i][0]) for i in range(len(lst_frame_left))]\n",
    "    lst_frame_body = [ast.literal_eval(lst_frame_body[i][0]) for i in range(len(lst_frame_body))]\n",
    "    lst_frame_face = [ast.literal_eval(lst_frame_face[i][0]) for i in range(len(lst_frame_face))]\n",
    "\n",
    "    # Extract x and y coordinates\n",
    "    right_x = [lst_frame_right[i][0] for i in range(len(lst_frame_right))]\n",
    "    right_y = [lst_frame_right[i][1] for i in range(len(lst_frame_right))]\n",
    "    left_x = [lst_frame_left[i][0] for i in range(len(lst_frame_left))]\n",
    "    left_y = [lst_frame_left[i][1] for i in range(len(lst_frame_left))]\n",
    "    body_x = [lst_frame_body[i][0] for i in range(len(lst_frame_body))]\n",
    "    body_y = [lst_frame_body[i][1] for i in range(len(lst_frame_body))]\n",
    "    face_x = [lst_frame_face[i][0] for i in range(len(lst_frame_face))]\n",
    "    face_y = [lst_frame_face[i][1] for i in range(len(lst_frame_face))]\n",
    "\n",
    "    # Create the plot\n",
    "    _, axis = plt.subplots(2, 2, constrained_layout=True, figsize=(8, 8))\n",
    "    if right_x != []:\n",
    "        axis[0, 0].scatter(right_x, right_y)\n",
    "        axis[0, 0].set_title(\"Right hand\")\n",
    "        draw_edges(axis[0, 0], lst_frame_right, edges[\"right_hand\"])\n",
    "\n",
    "    if left_x != []:\n",
    "        axis[0, 1].scatter(left_x, left_y)\n",
    "        axis[0, 1].set_title(\"Left hand\")\n",
    "        draw_edges(axis[0, 1], lst_frame_left, edges[\"left_hand\"])\n",
    "\n",
    "    if body_x != []:\n",
    "        axis[1, 0].scatter(body_x, body_y)\n",
    "        axis[1, 0].set_title(\"Body\")\n",
    "        draw_edges(axis[1, 0], lst_frame_body, edges[\"pose\"])\n",
    "\n",
    "    if right_x != [] and left_x != [] and body_x != []:\n",
    "        axis[1, 1].scatter(right_x, right_y)\n",
    "        axis[1, 1].scatter(left_x, left_y)\n",
    "        axis[1, 1].scatter(body_x, body_y)\n",
    "        axis[1, 1].set_title(\"All\")\n",
    "        draw_edges(axis[1, 1], lst_frame_right, edges[\"right_hand\"])\n",
    "        draw_edges(axis[1, 1], lst_frame_left, edges[\"left_hand\"])\n",
    "        draw_edges(axis[1, 1], lst_frame_body, edges[\"pose\"])\n",
    "\n",
    "    axis[0,0].yaxis.set_inverted(True)\n",
    "    axis[0,1].yaxis.set_inverted(True)\n",
    "    axis[1,0].yaxis.set_inverted(True)\n",
    "    axis[1,1].yaxis.set_inverted(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Using Zone__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732350140.612258  227507 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully captured frame: \u001b[91m\u001b[1m6\u001b[0m / 30"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 15:22:21.606 Python[7732:226853] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-11-23 15:22:21.606 Python[7732:226853] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully captured frame: \u001b[91m\u001b[1m30\u001b[0m / 30\n",
      "\u001b[1m\u001b[92mDone!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "operate(mp_holistic,holistic_model, mp_drawing, '/Users/theerat/Documents/tsl/sign-language-translator/New25/กิน/video_6.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph_drawer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello1 test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mgraph_drawer\u001b[0;34m(word, frames, edges)\u001b[0m\n\u001b[1;32m      6\u001b[0m lst_frame_left \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      7\u001b[0m lst_frame_body \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m lst_frame_face \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert string representations of lists to actual lists\u001b[39;00m\n\u001b[1;32m     11\u001b[0m lst_frame_right \u001b[38;5;241m=\u001b[39m [ast\u001b[38;5;241m.\u001b[39mliteral_eval(lst_frame_right[i][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lst_frame_right))]\n",
      "File \u001b[0;32m~/Documents/tsl/sign-language-translator/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/tsl/sign-language-translator/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/Documents/tsl/sign-language-translator/.venv/lib/python3.9/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "graph_drawer('hello1 test', 10, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __KeyPoint Folder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate2(mp_holistic, holistic_model, mp_drawing, file, folder_path, index):\n",
    "    if file == '.DS_Store':\n",
    "        return\n",
    "\n",
    "    vdo_path = os.path.join('/Users/theerat/Documents/tsl/sign-language-translator/fix_word', folder_path, file)\n",
    "    print(f'กำลัง keypoint {vdo_path}...')\n",
    "    output_dir = os.path.join('Landmarked', folder_path, str(index))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    vid = cv2.VideoCapture(vdo_path)\n",
    "    total_frame = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Avoid ZeroDivisionError if total_frame is less than 30\n",
    "    capture_interval = total_frame // 30 if total_frame >= 30 else 1\n",
    "\n",
    "    total_capture = 0\n",
    "\n",
    "    for i in range(total_frame):\n",
    "        if i % capture_interval == 0:\n",
    "            total_capture += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        vid.set(cv2.CAP_PROP_POS_FRAMES, total_capture * (total_frame / 30))\n",
    "        \n",
    "        # Convert the frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Process the frame with Mediapipe\n",
    "        results = holistic_model.process(image)\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        # Convert back to BGR for display\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Draw landmarks on the frame\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "        \n",
    "        # Display the processed frame (optional)\n",
    "        cv2.imshow(\"Display\", image)\n",
    "        \n",
    "        # Extract and save landmark data\n",
    "        left = results.left_hand_landmarks\n",
    "        right = results.right_hand_landmarks\n",
    "        upper = results.pose_landmarks\n",
    "        face = results.face_landmarks\n",
    "        hand_pos_df = extract_data([right, left, upper, face])\n",
    "        hand_pos_df.to_csv(os.path.join(output_dir, f'{total_capture - 1}.csv'), index=False)\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    # vid.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "    print(f\"\\nSuccessfully processed file: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_folder(folder_path):\n",
    "    for word_folder in os.listdir(folder_path):\n",
    "        if word_folder == '.DS_Store':\n",
    "            continue\n",
    "        print(f'กำลัง keypoint {word_folder}...')\n",
    "        files = os.listdir(f'/Users/theerat/Documents/tsl/sign-language-translator/fix_word/{word_folder}')\n",
    "        os.mkdir('Landmarked/' + word_folder)\n",
    "        for index,file in enumerate(files):\n",
    "            operate2(mp_holistic,holistic_model, mp_drawing, file,word_folder,index)\n",
    "    print(\"เสร็จหมดละจ้าา\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_folder(folder_path):\n",
    "    os.makedirs('Landmarked', exist_ok=True)\n",
    "    # word_path = ['ไก่ผัดกะเพรา', 'ชอบ', 'หัวเราะ', 'ไม่มี', 'นอน', 'ฉัน', 'นักเรียน', 'วิ่ง']\n",
    "    word_path = os.listdir(folder_path)\n",
    "    for word_folder in word_path:\n",
    "        if word_folder == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        print(f'Processing keypoints for {word_folder}...')\n",
    "        word_folder_path = os.path.join(folder_path, word_folder)\n",
    "        files = os.listdir(word_folder_path)\n",
    "        \n",
    "        os.makedirs(os.path.join('Landmarked', word_folder), exist_ok=True)\n",
    "        for index, file in enumerate(files):\n",
    "            operate2(mp_holistic,holistic_model, mp_drawing, file,word_folder,index)\n",
    "    \n",
    "    print(\"All processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ไก่ผัดกะเพรา', 'ชอบ', 'หัวเราะ', 'ไม่มี', 'นอน', 'ฉัน', 'นักเรียน', 'วิ่ง']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_path = ['ไก่ผัดกะเพรา', 'ชอบ', 'หัวเราะ', 'ไม่มี', 'นอน', 'ฉัน', 'นักเรียน', 'วิ่ง']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing keypoints for ฉัน...\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_38.mp4...\n",
      "\n",
      "Successfully processed file: video_38.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_10.mp4...\n",
      "\n",
      "Successfully processed file: video_10.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_11.mp4...\n",
      "\n",
      "Successfully processed file: video_11.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_39.mp4...\n",
      "\n",
      "Successfully processed file: video_39.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_13.mp4...\n",
      "\n",
      "Successfully processed file: video_13.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_12.mp4...\n",
      "\n",
      "Successfully processed file: video_12.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_16.mp4...\n",
      "\n",
      "Successfully processed file: video_16.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_17.mp4...\n",
      "\n",
      "Successfully processed file: video_17.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_8.mp4...\n",
      "\n",
      "Successfully processed file: video_8.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_15.mp4...\n",
      "\n",
      "Successfully processed file: video_15.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_29.mp4...\n",
      "\n",
      "Successfully processed file: video_29.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_28.mp4...\n",
      "\n",
      "Successfully processed file: video_28.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_14.mp4...\n",
      "\n",
      "Successfully processed file: video_14.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_9.mp4...\n",
      "\n",
      "Successfully processed file: video_9.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/ฉัน_test.mp4...\n",
      "\n",
      "Successfully processed file: ฉัน_test.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_49.mp4...\n",
      "\n",
      "Successfully processed file: video_49.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_48.mp4...\n",
      "\n",
      "Successfully processed file: video_48.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_46.mp4...\n",
      "\n",
      "Successfully processed file: video_46.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_47.mp4...\n",
      "\n",
      "Successfully processed file: video_47.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_45.mp4...\n",
      "\n",
      "Successfully processed file: video_45.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_50.mp4...\n",
      "\n",
      "Successfully processed file: video_50.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_44.mp4...\n",
      "\n",
      "Successfully processed file: video_44.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_40.mp4...\n",
      "\n",
      "Successfully processed file: video_40.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_41.mp4...\n",
      "\n",
      "Successfully processed file: video_41.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_43.mp4...\n",
      "\n",
      "Successfully processed file: video_43.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_42.mp4...\n",
      "\n",
      "Successfully processed file: video_42.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_4.mp4...\n",
      "\n",
      "Successfully processed file: video_4.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_19.mp4...\n",
      "\n",
      "Successfully processed file: video_19.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_31.mp4...\n",
      "\n",
      "Successfully processed file: video_31.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_25.mp4...\n",
      "\n",
      "Successfully processed file: video_25.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_24.mp4...\n",
      "\n",
      "Successfully processed file: video_24.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_30.mp4...\n",
      "\n",
      "Successfully processed file: video_30.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_18.mp4...\n",
      "\n",
      "Successfully processed file: video_18.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_5.mp4...\n",
      "\n",
      "Successfully processed file: video_5.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_7.mp4...\n",
      "\n",
      "Successfully processed file: video_7.mp4\n",
      "กำลัง keypoint /Users/theerat/Documents/tsl/sign-language-translator/fix_word/ฉัน/video_26.mp4...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mให้ใส่ชื่อ Folder ที่ต้องการ landmark โดยมันจะไปอยู่ในโฟลเดอร์ Landmarked อีกที ให้สร้างด้วยนะจ๊ะ\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mพอได้ไฟล์แล้ว เอาไปใส่ในไดร์ฟ อย่าอัปขึน git ไม่งั้น บู้มมมมมมมมมมม \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mlandmark_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/theerat/Documents/tsl/sign-language-translator/fix_word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 15\u001b[0m, in \u001b[0;36mlandmark_folder\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLandmarked\u001b[39m\u001b[38;5;124m'\u001b[39m, word_folder), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(files):\n\u001b[0;32m---> 15\u001b[0m         \u001b[43moperate2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp_holistic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mholistic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmp_drawing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mword_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll processing complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 35\u001b[0m, in \u001b[0;36moperate2\u001b[0;34m(mp_holistic, holistic_model, mp_drawing, file, folder_path, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Process the frame with Mediapipe\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Convert back to BGR for display\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/tsl/sign-language-translator/.venv/lib/python3.9/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/tsl/sign-language-translator/.venv/lib/python3.9/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ให้ใส่ชื่อ Folder ที่ต้องการ landmark โดยมันจะไปอยู่ในโฟลเดอร์ Landmarked อีกที ให้สร้างด้วยนะจ๊ะ\n",
    "พอได้ไฟล์แล้ว เอาไปใส่ในไดร์ฟ อย่าอัปขึน git ไม่งั้น บู้มมมมมมมมมมม \n",
    "\"\"\"\n",
    "landmark_folder('/Users/theerat/Documents/tsl/sign-language-translator/fix_word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Capture__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_recording = False\n",
    "\n",
    "def capture(word,count):\n",
    "    global stop_recording\n",
    "    webcam = cv2.VideoCapture(0)\n",
    "\n",
    "    if not webcam.isOpened():\n",
    "        # print(\"ไม่สามารถเปิด webcam ได้\")\n",
    "        exit()\n",
    "\n",
    "    frame_width = int(webcam.get(4))\n",
    "    frame_height = int(webcam.get(4))\n",
    "\n",
    "    fps = int(webcam.get(cv2.CAP_PROP_FPS))\n",
    "    print(f\"คำว่า : '{word}' ครั้งที่ {count}\")\n",
    "    # print(\"Webcam FPS:\", fps)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(f\"VDO/{word}/{word}_re_{count}.mp4\", fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while not stop_recording:\n",
    "        ret, frame = webcam.read()\n",
    "\n",
    "        if ret:\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "            font_path = \"SukhumvitSet-Medium.ttf\"\n",
    "            font = ImageFont.truetype(font_path, 120)\n",
    "            font2 = ImageFont.truetype(font_path, 50)\n",
    "\n",
    "            draw.text((50, 50), word, font=font, fill=(255, 255, 255))\n",
    "            draw.text((50,200), f'คลิปที่ : {count}', font=font, fill=(255, 255, 255))\n",
    "            draw.text((1500,50), 'กด q เมื่อบันทึกเสร็จ', font=font2, fill=(255, 255, 255))\n",
    "            draw.text((1500,200), 'กด d เพื่อยกเลิก', font=font2, fill=(255, 255, 255))\n",
    "\n",
    "            frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            cv2.imshow(\"Webcam\", frame)\n",
    "            out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(100) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    webcam.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_sl(word):\n",
    "    global stop_recording\n",
    "    ex_vdo = cv2.VideoCapture(f'VDO/{word}/{word}_test.mp4')\n",
    "    while True:\n",
    "        ret, frame = ex_vdo.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"ไม่สามารถอ่านไฟล์ได้\")\n",
    "            break\n",
    "\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "        font_path = \"SukhumvitSet-Medium.ttf\"\n",
    "        font = ImageFont.truetype(font_path, 50)\n",
    "\n",
    "        text_position = (50, 50)\n",
    "        draw.text(text_position, word, font=font, fill=(255, 255, 255))\n",
    "\n",
    "        frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('Example Video', frame)\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    ex_vdo.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    for i in range(1,30):\n",
    "        capture(word,i)\n",
    "        time.sleep(5)\n",
    "        if cv2.waitKey(100) & 0xFF == ord('d'):\n",
    "            stop_recording = True\n",
    "            break\n",
    "        \n",
    "    print('เสร็จแล้ววว')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to stop capturing videos.\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_1.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_2.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_3.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_4.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_5.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_6.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_7.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_8.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_9.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_10.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_11.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_12.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_13.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_14.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_15.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_16.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_17.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_18.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_19.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_20.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_21.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_22.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_23.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_24.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_25.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_26.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_27.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_28.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_29.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_30.mp4\n",
      "Started new video: /Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่/video_31.mp4\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create a directory to save videos\n",
    "output_dir = \"/Users/theerat/Documents/tsl/sign-language-translator/New25/อยู่\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Open the video capture (0 for default webcam, or provide a video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video source.\")\n",
    "    exit()\n",
    "\n",
    "# Video parameters\n",
    "fps = 20  # Frames per second\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 files\n",
    "\n",
    "video_count = 0\n",
    "start_time = time.time()\n",
    "out = None\n",
    "\n",
    "print(\"Press 'q' to stop capturing videos.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture video frame.\")\n",
    "        break\n",
    "\n",
    "    # Get elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Start a new video file every 4 seconds\n",
    "    if elapsed_time >= 4 or out is None:\n",
    "        if out:  # Release the current video writer\n",
    "            out.release()\n",
    "        \n",
    "        # Create a new video file\n",
    "        video_count += 1\n",
    "        video_filename = os.path.join(output_dir, f\"video_{video_count}.mp4\")\n",
    "        out = cv2.VideoWriter(video_filename, fourcc, fps, (frame_width, frame_height))\n",
    "        print(f\"Started new video: {video_filename}\")\n",
    "        start_time = time.time()  # Reset the timer\n",
    "\n",
    "    # Write the current frame to the video file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the video count and seconds count on the video feed\n",
    "    video_text = f\"Recording Video: {video_count}\"\n",
    "    seconds_text = f\"Elapsed Time: {int(elapsed_time)} sec\"\n",
    "\n",
    "    # ฟอนต์และการตั้งค่า\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 2  # เพิ่มขนาดฟอนต์\n",
    "    font_color = (0, 255, 0)  # สีเขียว\n",
    "    thickness = 3  # เพิ่มความหนา\n",
    "\n",
    "    # ตำแหน่งข้อความ\n",
    "    video_position = (10, 100)  # ตำแหน่งของข้อความ Video count\n",
    "    seconds_position = (10, 200)  # ตำแหน่งของข้อความ Elapsed time\n",
    "\n",
    "    # วาดข้อความลงบนเฟรม\n",
    "    cv2.putText(frame, video_text, video_position, font, font_scale, font_color, thickness)\n",
    "    cv2.putText(frame, seconds_text, seconds_position, font, font_scale, font_color, thickness)\n",
    "\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow(\"Video Feed\", frame)\n",
    "\n",
    "    # Check for the 'q' key to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "# Release the video writer and video capture\n",
    "if out:\n",
    "    out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
